{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "1Ay6NL6a37Yn",
        "iMfQzupt6OBP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zebreu/DeorphaNN/blob/main/DeorphaNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#location to save results\n",
        "save_to = '/content/'"
      ],
      "metadata": {
        "id": "HQwMlWGQMvUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "%%capture\n",
        "!pip uninstall torch -y\n",
        "!pip install torch==2.4.0\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "!pip install optuna\n",
        "!pip install numpy-indexed\n",
        "\n",
        "import glob\n",
        "import warnings\n",
        "import numpy as np\n",
        "import numpy_indexed as npi\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, average_precision_score\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATv2Conv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool\n",
        "from torch_geometric.nn import aggr\n",
        "from torch_geometric.nn.norm import GraphNorm\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import optuna\n",
        "\n",
        "torch.manual_seed(111)\n",
        "from huggingface_hub import hf_hub_download, list_repo_files\n",
        "import h5py\n",
        "import random\n"
      ],
      "metadata": {
        "id": "JH1jGME8t_lx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import files from HuggingFace repo\n",
        "%%capture\n",
        "repo_id = \"lariferg/DeorphaNN\"\n",
        "all_files = list_repo_files(repo_id, repo_type=\"dataset\")\n",
        "\n",
        "pdbs_paths = sorted(\n",
        "    hf_hub_download(repo_id, f, repo_type=\"dataset\")\n",
        "    for f in all_files\n",
        "    if f.startswith(\"DeorphaNN_training/nov7relaxed\") and f.endswith(\".parquet\")\n",
        ")\n",
        "\n",
        "labels = hf_hub_download(\n",
        "    repo_id,\n",
        "    next(f for f in all_files if f.startswith(\"DeorphaNN_training/\") and f.endswith(\"Dataset_Labels_full - Sheet1.csv\")),\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "beetsdata = pd.read_csv(labels)\n",
        "\n",
        "\n",
        "min_dis = hf_hub_download(\n",
        "    repo_id,\n",
        "    next(f for f in all_files if f.startswith(\"DeorphaNN_training/\") and f.endswith(\"mindistance_active_bias.csv\")),\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "outsidepocket = pd.read_csv(min_dis)\n",
        "\n",
        "\n",
        "new_contacts_paths = sorted(\n",
        "    hf_hub_download(repo_id, f, repo_type=\"dataset\")\n",
        "    for f in all_files\n",
        "    if f.startswith(\"DeorphaNN_training/nov7relaxed\") and f.endswith(\"_arpeggio_contacts3.parquet\")\n",
        ")\n",
        "\n",
        "hdfs_int = sorted(\n",
        "    hf_hub_download(repo_id, f, repo_type=\"dataset\")\n",
        "    for f in all_files\n",
        "    if f.startswith(\"pair_representations/\") and f.endswith(\"_interaction.h5\")\n",
        ")\n",
        "\n",
        "\n",
        "hdfs_t = sorted(\n",
        "    hf_hub_download(repo_id, f, repo_type=\"dataset\")\n",
        "    for f in all_files\n",
        "    if f.startswith(\"pair_representations/\") and f.endswith(\"T.h5\")\n",
        ")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MCY5oroxvu1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preparing data"
      ],
      "metadata": {
        "id": "1Ay6NL6a37Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpcrs = []\n",
        "peptides = []\n",
        "plddts = []\n",
        "paths = []\n",
        "plddt_peptides = []\n",
        "plddt_gpcrs = []\n",
        "plddt_atoms = []\n",
        "pdb_frames = dict()\n",
        "for pdbs in pdbs_paths:\n",
        "    print(pdbs)\n",
        "    pdbs = pd.read_parquet(pdbs)\n",
        "    for key, st in pdbs.groupby('path'):\n",
        "        if 'amber_r_' in key:\n",
        "            original_key = key\n",
        "            key = key.replace('amber_r_', '')\n",
        "        gpcrs.append(key.split('/')[-1].split('_')[0])\n",
        "        peptides.append(key.split('/')[-1].split('_')[1])\n",
        "        plddt_peptides.append(st[st['chain_id'] == 'B'].groupby('residue_seq_id')['b_factor'].first().mean())\n",
        "        plddt_gpcrs.append(st[st['chain_id'] == 'A'].groupby('residue_seq_id')['b_factor'].first().mean())\n",
        "        plddt_atoms.append(st[st['chain_id'] == 'B']['b_factor'].mean())\n",
        "        paths.append(original_key)\n",
        "        pdb_frames[original_key] = st\n",
        "    del pdbs"
      ],
      "metadata": {
        "id": "PmhgP9djAzN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = pd.DataFrame({'path': paths, 'gpcr': gpcrs, 'peptide': peptides, 'plddt_peptides': plddt_peptides, 'plddt_gpcrs': plddt_gpcrs, 'plddt_atoms': plddt_atoms})\n",
        "merged = pd.merge(beetsdata, st, how='left', left_on=['GPCR name', 'Peptide'], right_on=['gpcr', 'peptide'])\n",
        "merged['gpcr_family'] = merged['GPCR name'].str[:-2]\n",
        "merged['y'] = merged['binds'].apply(lambda x: 1 if x == True else 0)"
      ],
      "metadata": {
        "id": "mMSfYq_4AxOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits = merged[merged['gpcr'].isna() == False]\n",
        "gpcrs_lens = []\n",
        "peps_lens = []\n",
        "for index, st in gpcr_hits.iterrows():\n",
        "    pdb = pdb_frames[st['path']]\n",
        "    rec = pdb[pdb['chain_id'] == 'A']\n",
        "    pep = pdb[pdb['chain_id'] == 'B']\n",
        "    gpcrs_lens.append(rec['residue_seq_id'].max())\n",
        "    peps_lens.append(pep['residue_seq_id'].max())\n",
        "gpcr_hits['gpcr_len'] = gpcrs_lens\n",
        "gpcr_hits['pep_len'] = peps_lens"
      ],
      "metadata": {
        "id": "u1JCE-85UqHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outsidepocket['pair'] = outsidepocket['GPCR name']+'_'+outsidepocket['Peptide']"
      ],
      "metadata": {
        "id": "cxIjr4boVETU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits = gpcr_hits[-gpcr_hits['pair'].isin(set(outsidepocket['pair'].values))]"
      ],
      "metadata": {
        "id": "C3koRdr8VNRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allcontacts_new = []\n",
        "for cpath_new in new_contacts_paths:\n",
        "    allcontacts_new.append(pd.read_parquet(cpath_new))\n",
        "allcontacts_new = pd.concat(allcontacts_new)"
      ],
      "metadata": {
        "id": "k7giqa9_3NMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(allcontacts_new)"
      ],
      "metadata": {
        "id": "7EgWw0X8gt0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allcontacts_new"
      ],
      "metadata": {
        "id": "QXva0R4G3wYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_interactions = allcontacts_new.groupby('gpcr_peptide')['contacts'].apply(lambda x: sum(len(c) for c in x)).reset_index()\n",
        "total_interactions.columns = ['gpcr_peptide', 'total_interactions']\n"
      ],
      "metadata": {
        "id": "LRIGX2yE5VsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_interactions"
      ],
      "metadata": {
        "id": "BzDLh7734QYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits"
      ],
      "metadata": {
        "id": "p5ZPFu254ZOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits_bonds = pd.merge(\n",
        "    gpcr_hits,\n",
        "    total_interactions,\n",
        "    how='left',\n",
        "    left_on='pair',\n",
        "    right_on='gpcr_peptide'\n",
        ")\n",
        "\n",
        "# Optionally drop the redundant 'gpcr_peptide' column\n",
        "gpcr_hits_bonds = gpcr_hits_bonds.drop(columns=['gpcr_peptide'])\n"
      ],
      "metadata": {
        "id": "pdSVLZbO5uXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits_bonds"
      ],
      "metadata": {
        "id": "LypY3r2E33rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions = dict()\n",
        "for key, st in allcontacts_new.groupby('gpcr_peptide'):\n",
        "    interactions[key] = st"
      ],
      "metadata": {
        "id": "2hm2aOlHVvbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits_interaction_edges = dict()\n",
        "for index, g in gpcr_hits.iterrows():\n",
        "    pdb = pdb_frames[g['path']].copy()\n",
        "    if g['path'] not in interactions:\n",
        "        continue\n",
        "    bonds = interactions[g['path']]\n",
        "\n",
        "    gpcr_len = g['gpcr_len']\n",
        "\n",
        "    # they're 1-indexed so -1\n",
        "    bonds['source'] = bonds['bgn'].apply(lambda x: x['auth_seq_id'] if x['auth_asym_id'] == \"A\" else x['auth_seq_id'] + gpcr_len) - 1\n",
        "    bonds['target'] = bonds['end'].apply(lambda x: x['auth_seq_id'] if x['auth_asym_id'] == \"A\" else x['auth_seq_id'] + gpcr_len) - 1\n",
        "    bonds = bonds.groupby(['source', 'target'])['contact'].agg(lambda x: {bondtype for array in x for bondtype in array}).reset_index()\n",
        "    sources = bonds['source'].values\n",
        "    targets = bonds['target'].values\n",
        "\n",
        "    h_edge_index = np.vstack([sources,targets])\n",
        "    key = g['gpcr']+'_'+g['peptide']\n",
        "    gpcr_hits_interaction_edges[key] = h_edge_index"
      ],
      "metadata": {
        "id": "ruOTzucUG1eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits_interaction_edges_new = dict()\n",
        "for _, row in allcontacts_new.iterrows():\n",
        "    key = row['gpcr_peptide']\n",
        "    contacts = row['contacts']\n",
        "    # Check if contacts is None, NaN, or empty\n",
        "    if contacts is None or len(contacts) == 0:\n",
        "        # create empty 2x0 array\n",
        "        #gpcr_hits_interaction_edges_new[key] = np.empty((2,0), dtype=int)\n",
        "        continue\n",
        "    # Stack the pairs vertically and transpose\n",
        "    arr = np.vstack(contacts).T  # shape: 2 x N_pairs\n",
        "    arr -= 1 #convert 1-indexed to 0-indexed\n",
        "    gpcr_hits_interaction_edges_new[key] = arr"
      ],
      "metadata": {
        "id": "rGsDi2lvHU1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(gpcr_hits_interaction_edges_new)"
      ],
      "metadata": {
        "id": "CGmm9AA0SD4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(interactions)"
      ],
      "metadata": {
        "id": "twO6U7VVWSUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits_interaction_edges_new['DMSR-5-1_FLP-1-7']"
      ],
      "metadata": {
        "id": "bp6Agm8KGi5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lens = gpcr_hits.groupby(['GPCR name'])['gpcr_len'].first()"
      ],
      "metadata": {
        "id": "juQL347cyOoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(hdfs_int)"
      ],
      "metadata": {
        "id": "8idNodY-lcNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_map_interaction = dict()\n",
        "emb_map_interaction_gpcrindex = dict()\n",
        "pairmissed = []\n",
        "for hdf in hdfs_int:\n",
        "    with h5py.File(hdf, \"r\") as f:\n",
        "        keys = list(f.keys())\n",
        "        print(keys)\n",
        "        gpcr = keys[0].split('_')[0]\n",
        "        for k in keys:\n",
        "            try:\n",
        "                array = np.nan_to_num(f[k][()],0)\n",
        "                peptide = k.split('_')[1]\n",
        "                mapkey = gpcr+'_'+peptide\n",
        "                indices_to_keep = set()\n",
        "                maximum = lens[gpcr]\n",
        "                indices_to_keep.update(set(gpcr_hits_interaction_edges_new[mapkey][0]))\n",
        "                indices_to_keep.update(set(gpcr_hits_interaction_edges_new[mapkey][1]))\n",
        "                indices_to_keep = sorted([i for i in indices_to_keep if i < maximum])\n",
        "                emb_map_interaction[mapkey] = array[:,indices_to_keep,:]\n",
        "                emb_map_interaction_gpcrindex[mapkey] = np.array(indices_to_keep)\n",
        "                print(\"success \"+k)\n",
        "            except:\n",
        "                pairmissed.append(k)\n",
        "                print(\"missed \"+k)\n",
        "                continue"
      ],
      "metadata": {
        "id": "6U1NZONkGixM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(emb_map_interaction)"
      ],
      "metadata": {
        "id": "YSTxksl_o2_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairmissed)"
      ],
      "metadata": {
        "id": "tZJoSCHDlSuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_peptide_arrays = []\n",
        "peptide_keys = []\n",
        "all_gpcr_arrays = []\n",
        "gpcr_keys = []\n",
        "\n",
        "for hdf in hdfs_t:\n",
        "    with h5py.File(hdf, \"r\") as f:\n",
        "        arrays = []\n",
        "        keys = list(f.keys())\n",
        "\n",
        "        for k in keys:\n",
        "            arrays.append(f[k][()])\n",
        "        if \"_pep_T\" in hdf:\n",
        "            all_peptide_arrays.append(arrays)\n",
        "            peptide_keys.append(keys)\n",
        "        if \"_gpcr_T\" in hdf:\n",
        "            all_gpcr_arrays.append(arrays)\n",
        "            gpcr_keys.append(keys)"
      ],
      "metadata": {
        "id": "fPh3a-EAJOmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_map_gpcr = dict()\n",
        "for keys, arrays in zip(gpcr_keys, all_gpcr_arrays):\n",
        "    try:\n",
        "        gpcr = keys[0].split('_')[0]\n",
        "\n",
        "        for i,array in enumerate(arrays):\n",
        "            peptide = keys[i].split('_')[1]\n",
        "            emb_map_gpcr[gpcr+'_'+peptide] = array\n",
        "    except:\n",
        "        print('oops')\n",
        "        continue"
      ],
      "metadata": {
        "id": "mqcL6TEONGpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_map_peptide = dict()\n",
        "for keys, arrays in zip(peptide_keys, all_peptide_arrays):\n",
        "    try:\n",
        "        gpcr = keys[0].split('_')[0]\n",
        "\n",
        "        for i,array in enumerate(arrays):\n",
        "            peptide = keys[i].split('_')[1]\n",
        "            emb_map_peptide[gpcr+'_'+peptide] = array\n",
        "\n",
        "    except:\n",
        "        print('oops')\n",
        "        continue"
      ],
      "metadata": {
        "id": "kn8aVRBUNGbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embst = pd.DataFrame({'gpcr_keys': [kk for k in gpcr_keys for kk in k ], 'gpcr_embedding': [aa.mean(axis=0) for a in all_gpcr_arrays for aa in a ], 'peptide_keys': [kk for k in peptide_keys for kk in k], 'peptide_embedding': [aa.mean(axis=0) for a in all_peptide_arrays for aa in a]})"
      ],
      "metadata": {
        "id": "G73Q_51mNY3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embst['peptide'] = embst['peptide_keys'].apply(lambda x: x.split('_')[1])\n",
        "embst['gpcr'] = embst['gpcr_keys'].apply(lambda x: x.split('_')[0])"
      ],
      "metadata": {
        "id": "hDXxFPkQJAho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcrweight = 1/gpcr_hits.groupby(['gpcr']).agg({'y': 'sum'}).sort_values(by='y')"
      ],
      "metadata": {
        "id": "b6at-weWJAZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcrweight"
      ],
      "metadata": {
        "id": "tmztq8A4xQCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits"
      ],
      "metadata": {
        "id": "tNM1ZY7vb0AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subgraphing = True\n",
        "subgraph_hops = 1\n",
        "with_edge_weights = True\n",
        "missed = []\n",
        "all_graphs = []\n",
        "for index, g in gpcr_hits.iterrows():\n",
        "    mapkey = g['gpcr']+'_'+g['peptide']\n",
        "    if mapkey not in gpcr_hits_interaction_edges_new:\n",
        "        missed.append((mapkey, g['y']))\n",
        "        continue\n",
        "    gpcr_len = g['gpcr_len']\n",
        "    h_edge_index = gpcr_hits_interaction_edges_new[g['gpcr']+'_'+g['peptide']]\n",
        "    xg = emb_map_gpcr[g['gpcr']+'_'+g['peptide']]\n",
        "    xp = emb_map_peptide[g['gpcr']+'_'+g['peptide']]\n",
        "    x = np.concatenate([xg, xp])\n",
        "    x = torch.from_numpy(x).type(torch.float32)\n",
        "    pep_edge_index = np.vstack([np.array(range(g['gpcr_len'], len(x)-1)), np.array(range(g['gpcr_len']+1, len(x)))])\n",
        "    edge_index = torch.cat([torch.from_numpy(h_edge_index), torch.from_numpy(pep_edge_index)], dim=1)\n",
        "    if with_edge_weights:\n",
        "        if mapkey not in emb_map_interaction:\n",
        "            missed.append((mapkey, g['y']))\n",
        "            continue\n",
        "        edgefeatures = emb_map_interaction[mapkey]\n",
        "        edgeindices = emb_map_interaction_gpcrindex[mapkey]\n",
        "        sources = npi.remap(h_edge_index[0], edgeindices, np.arange(len(edgeindices)))\n",
        "        targets = npi.remap(h_edge_index[1], edgeindices, np.arange(len(edgeindices)))\n",
        "        sourcewherever = np.where(sources >= gpcr_len)[0]\n",
        "        targetwherever = np.where(targets < gpcr_len)[0]\n",
        "        newsources = np.array(sources)\n",
        "        newtargets = np.array(targets)\n",
        "        newsources[sourcewherever] = targets[sourcewherever]\n",
        "        newtargets[targetwherever] = sources[targetwherever]\n",
        "        newtargets -= gpcr_len\n",
        "        edge_attrs = edgefeatures[newtargets, newsources, :]\n",
        "        pep_edge_attrs = np.ones(shape=(len(pep_edge_index[0]),128))*edge_attrs.mean(axis=0)\n",
        "        edge_attrs = torch.from_numpy(edge_attrs).type(torch.float32)\n",
        "        edge_attrs = torch.cat([edge_attrs, torch.from_numpy(pep_edge_attrs)], dim=0)\n",
        "    if with_edge_weights:\n",
        "        # convert to undirected first, so hops are symmetric\n",
        "        edge_index, edge_attrs = torch_geometric.utils.to_undirected(edge_index, edge_attrs, reduce='mean')\n",
        "    if subgraphing:\n",
        "        to_keep = torch.tensor([i for i in range(gpcr_len, len(x))]) #hopping from peptide nodes\n",
        "        # to_keep = torch.unique(torch.from_numpy(h_edge_index[0])) #hopping from gpcr nodes\n",
        "\n",
        "        nodes, edges, _, _ = torch_geometric.utils.k_hop_subgraph(to_keep, subgraph_hops, edge_index, relabel_nodes=True, num_nodes=len(x))\n",
        "        # mask = (nodes >= gpcr_len) | (torch.isin(nodes, to_keep))\n",
        "        # nodes = nodes[mask]\n",
        "        if with_edge_weights:\n",
        "            edges, new_edge_attrs = torch_geometric.utils.subgraph(nodes, edge_index, edge_attrs, relabel_nodes=True)\n",
        "            # edges, new_edge_attrs = torch_geometric.utils.to_undirected(edges, new_edge_attrs, reduce='mean')\n",
        "            graph = Data(x=x[nodes], edge_index=edges, edge_attr=new_edge_attrs, y=torch.tensor(g['y']))\n",
        "        else:\n",
        "            graph = Data(x=x[nodes], edge_index=edges, y=torch.tensor(g['y']))\n",
        "    else:\n",
        "        graph = Data(x=x, edge_index=edge_index, y=torch.tensor(g['y']))\n",
        "    graph.peptide = g['peptide']\n",
        "    graph.gpcr = g['gpcr']\n",
        "    graph.gpcr_family = g['gpcr_family']\n",
        "    #graph.zscore = g['modifiedzscore']\n",
        "    gpcrw = gpcrweight.loc[g['gpcr']].iloc[0]\n",
        "    all_graphs.append({'graph': graph, 'peptide':g['peptide'], 'gpcr':g['gpcr'], 'gpcr_family': g['gpcr_family'], 'y': g['y'], 'gpcrweight': gpcrw})\n"
      ],
      "metadata": {
        "id": "LmeVM9M0Y3a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_graphs)"
      ],
      "metadata": {
        "id": "wbHbH-BdbjHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(missed)"
      ],
      "metadata": {
        "id": "RxvZs4Ujbrrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(missed)"
      ],
      "metadata": {
        "id": "GczhvVYZ9g5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "iMfQzupt6OBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch) # edge_attr\n",
        "        loss = criterion(logits, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "def train_weighted(model, criterion, optimizer, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch) # edge_attr\n",
        "        loss = criterion(logits, data.y)\n",
        "        loss = (loss*data.gpcrweight).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * data.num_graphs\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_roc(model, criterion, loader):\n",
        "     model.eval()\n",
        "     aucs = 0\n",
        "     total = len(loader.dataset)\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.edge_attr, data.batch) # edge_attr\n",
        "         aucs += roc_auc_score(data.y.detach().cpu(), torch.softmax(out.detach(),dim=1).cpu()[:, 1])*(len(out)/total)\n",
        "     return aucs\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_without_crash(model, criterion, loader):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    atrues = []\n",
        "    for data in loader:\n",
        "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch) # data.edge_attr\n",
        "        all_logits.append(logits.cpu().detach()[:,1])\n",
        "        atrues.append(data.y.cpu())\n",
        "    return roc_auc_score(np.concatenate(atrues), np.concatenate(all_logits))\n",
        "\n",
        "@torch.no_grad()\n",
        "def nope_test_without_crash(model, criterion, loader):\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    atrues = []\n",
        "    for data in loader:\n",
        "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch) # data.edge_attr\n",
        "        all_logits.append(torch.sigmoid(logits.squeeze()).cpu().detach())\n",
        "        atrues.append(data.y.cpu())\n",
        "    return roc_auc_score(np.concatenate(atrues), np.concatenate(all_logits))\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, criterion, loader):\n",
        "    model.eval()\n",
        "\n",
        "    total_correct = 0\n",
        "    for data in loader:\n",
        "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch) # data.edge_attr\n",
        "        pred = logits.argmax(dim=-1)\n",
        "        total_correct += int((pred == data.y).sum())\n",
        "\n",
        "    return total_correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "117kva8AYP_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_to_cuda(g):\n",
        "    g.x = g.x.cuda()\n",
        "    g.edge_index = g.edge_index.cuda()\n",
        "    g.edge_attr = g.edge_attr.cuda().type(torch.float32)\n",
        "    g.y = g.y.cuda()\n",
        "    return g"
      ],
      "metadata": {
        "id": "IkQU4t9MZMZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn.norm import LayerNorm, BatchNorm\n",
        "from torch_geometric.nn import global_add_pool\n",
        "from torch_geometric.nn import aggr\n",
        "import sklearn\n",
        "class PeptideGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, input_channels=4, gatheads=10, gatdropout=0.5, finaldropout=0.5):\n",
        "        super(PeptideGNN, self).__init__()\n",
        "        self.finaldropout = finaldropout\n",
        "        torch.manual_seed(111)\n",
        "        self.norm = BatchNorm(input_channels)\n",
        "        self.conv1 = GATv2Conv(input_channels, hidden_channels, dropout=gatdropout, heads=gatheads, concat=False, edge_dim=128)\n",
        "        self.pooling = global_mean_pool\n",
        "        self.lin = Linear(hidden_channels, 2)\n",
        "    def forward(self, x, edge_index, edge_attr, batch, hidden=False):\n",
        "        x = self.norm(x)\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = x.relu()\n",
        "        if hidden:\n",
        "            return x\n",
        "        x = self.pooling(x, batch)\n",
        "        x = F.dropout(x, p=self.finaldropout, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8I5F1tYFZMRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpcr_hits.groupby(['gpcr']).agg({'y': 'sum'}).sort_values(by='y')"
      ],
      "metadata": {
        "id": "-fBV-4YQZa3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for g in all_graphs:\n",
        "    g['graph'].gpcrweight = torch.tensor(g['gpcrweight']).cuda()"
      ],
      "metadata": {
        "id": "2DjxmEYyZq59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original splits (gpcr)\n",
        "# validation_peptides = [['NPR-43', 'CKR-1', 'NPR-39', 'AEX-2', 'DMSR-2', 'NPR-41'],\n",
        "# ['NPR-11', 'SPRR-2', 'SPRR-1', 'NPR-10', 'DMSR-3', 'GNRR-6'],\n",
        "# ['NPR-5', 'DMSR-8', 'NPR-2', 'FRPR-9', 'NPR-42', 'NPR-32'],\n",
        "# ['FRPR-8', 'NPR-40', 'FRPR-16', 'NPR-1', 'FRPR-6', 'FRPR-4'],\n",
        "# ['NPR-6', 'NMUR-2', 'FRPR-7', 'NPR-13', 'FRPR-19', 'TRHR-1'],\n",
        "# ['GNRR-1', 'FRPR-18', 'NPR-37', 'PDFR-1', 'FRPR-3'],\n",
        "# ['NPR-22', 'EGL-6', 'CKR-2', 'NMUR-1', 'NPR-4', 'FRPR-15'],\n",
        "# ['NPR-24', 'SEB-3', 'DMSR-6', 'NPR-12', 'DMSR-7'],\n",
        "# ['GNRR-3', 'NPR-35', 'TKR-2', 'NTR-1', 'DMSR-5'],\n",
        "# ['NPR-8', 'DMSR-1', 'NPR-3', 'TKR-1']]\n",
        "\n",
        "#phylogenetic splits (gpcr)\n",
        "validation_peptides = [['FRPR-16', 'FRPR-18', 'FRPR-4', 'FRPR-6', 'NPR-22', 'NMUR-2'],\n",
        "['AEX-2', 'DMSR-5', 'DMSR-6', 'DMSR-7', 'DMSR-8','NPR-32'],\n",
        "['FRPR-7', 'FRPR-7', 'NPR-6', 'GNRR-3', 'EGL-6'],\n",
        "['TKR-1', 'TKR-2', 'DMSR-1', 'DMSR-2', 'NPR-40', 'GNRR-6'],\n",
        "['NPR-42', 'FRPR-9', 'FRPR-15', 'FRPR-19', 'NMUR-1'],\n",
        "['NPR-8', 'NPR-24', 'NPR-37', 'NPR-43', 'FRPR-3'],\n",
        "['FRPR-8', 'GNRR-1', 'SPRR-1', 'SPRR-2', 'NPR-11', 'NPR-12'],\n",
        "['NPR-41', 'NPR-1', 'NPR-2', 'NPR-3', 'PDFR-1', 'NTR-1'],\n",
        "['TRHR-1', 'NPR-35', 'NPR-13', 'NPR-5', 'SEB-3'],\n",
        "['DMSR-3', 'NPR-10', 'NPR-4', 'NPR-39', 'CKR-1', 'CKR-2']]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nX8NXJkfJjvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hpt_peptides = validation_peptides[-2:]+validation_peptides[0:-2]"
      ],
      "metadata": {
        "id": "YtDFOCuRJkRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_graphs"
      ],
      "metadata": {
        "id": "K7yM6F4hbSA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_logits = []\n",
        "test_labels = []\n",
        "candidatesgnn = []\n",
        "hitmapgnn = []\n",
        "subsampling_factor = 4\n",
        "average_precisions = dict()\n",
        "hpt_results = []\n",
        "\n",
        "for validation_peptide,hpt_peptide in zip(validation_peptides, hpt_peptides):\n",
        "    training_graphs = [g['graph'] for g in all_graphs if g['gpcr_family'] not in validation_peptide and g['y'] == 1]\n",
        "    hpt_training_graphs = [g['graph'] for g in all_graphs if g['gpcr_family'] not in validation_peptide and g['gpcr_family'] not in hpt_peptide and g['y'] == 1]\n",
        "    hpt_to_shuffle = [g['graph'] for g in all_graphs if g['gpcr_family'] not in validation_peptide and g['gpcr_family'] not in hpt_peptide and g['y'] == 0]\n",
        "    to_shuffle = [g['graph'] for g in all_graphs if g['gpcr_family'] not in validation_peptide and g['y'] == 0]\n",
        "    random.Random(111).shuffle(to_shuffle)\n",
        "    random.Random(111).shuffle(hpt_to_shuffle)\n",
        "    trainings = []\n",
        "    hpt_trainings = []\n",
        "    for i in range(20):\n",
        "        random.Random(i).shuffle(to_shuffle)\n",
        "        trainings.append(training_graphs + to_shuffle[0:len(training_graphs)*subsampling_factor])\n",
        "        trainings = [list(map(move_to_cuda, h)) for h in trainings]\n",
        "        random.Random(i).shuffle(hpt_to_shuffle)\n",
        "        hpt_trainings.append(hpt_training_graphs + hpt_to_shuffle[0:len(hpt_training_graphs)*subsampling_factor])\n",
        "        hpt_trainings = [list(map(move_to_cuda, h)) for h in hpt_trainings]\n",
        "    validation_graphs = [g['graph'] for g in all_graphs if g['gpcr_family'] in validation_peptide]\n",
        "    hpt_validation_graphs = [g['graph'] for g in all_graphs if g['gpcr_family'] in hpt_peptide]\n",
        "    validation_graphs = list(map(move_to_cuda, validation_graphs))\n",
        "    hpt_validation_graphs = list(map(move_to_cuda, hpt_validation_graphs))\n",
        "    test_loader = DataLoader(validation_graphs, batch_size=256, shuffle=False)\n",
        "    hpt_test_loader = DataLoader(hpt_validation_graphs, batch_size=256, shuffle=False)\n",
        "    hpt_maps = []\n",
        "    hpt_logits = []\n",
        "    hpt_labels = []\n",
        "    def objective(trial):\n",
        "        hidden_channels = trial.suggest_int('hidden_units', 50, 100)\n",
        "        batch_size = trial.suggest_int('batch_size', 50, 200)\n",
        "        lr = 0.0005\n",
        "        model = PeptideGNN(hidden_channels, input_channels=128).cuda()\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.5, reduction='none')\n",
        "        for epoch in range(1, 30):\n",
        "            training_graphs = random.Random(epoch+111).choice(hpt_trainings)\n",
        "            train_loader = DataLoader(training_graphs, batch_size=batch_size, shuffle=True)\n",
        "            loss = train_weighted(model, criterion, optimizer, train_loader)\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        atrues = []\n",
        "        with torch.no_grad():\n",
        "            for data in hpt_test_loader:\n",
        "                logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "\n",
        "                if torch.isnan(data.x).any():\n",
        "                    print(f\"NaN in node features for {hpt_peptide}\")\n",
        "                if torch.isnan(data.edge_attr).any():\n",
        "                    print(f\"NaN in edge attributes for {hpt_peptide}\")\n",
        "                if torch.isinf(data.x).any() or torch.isinf(data.edge_attr).any():\n",
        "                    print(f\"Infinite values in data for {hpt_peptide}\")\n",
        "\n",
        "                # ðŸ§© Check for NaNs in model output\n",
        "                if torch.isnan(logits).any():\n",
        "                    print(f\"NaN detected in model output during Optuna eval for {hpt_peptide}\")\n",
        "                    continue  # skip this batch safely\n",
        "\n",
        "                all_logits.append(logits.cpu().detach()[:,1])\n",
        "                atrues.append(data.y.cpu())\n",
        "        hpt_logits.append(np.concatenate(all_logits))\n",
        "        hpt_labels.append(np.concatenate(atrues))\n",
        "        hpt_average_precisions = []\n",
        "        val_gpcr = [g.gpcr for g in hpt_validation_graphs]\n",
        "        val_peptide = [g.peptide for g in hpt_validation_graphs]\n",
        "        result = pd.DataFrame(zip(hpt_labels[-1], hpt_logits[-1], val_gpcr, val_peptide))\n",
        "        for gpcr, r in result.groupby(2):\n",
        "            if r[0].sum() > 0:\n",
        "                if np.isnan(r[0]).any():\n",
        "                    print(f\"NaNs in TRUE labels for group: {gpcr}\")\n",
        "                if np.isnan(r[1]).any():\n",
        "                    print(f\"NaNs in PREDICTIONS for group: {gpcr}\")\n",
        "                hpt_average_precisions.append(sklearn.metrics.average_precision_score(r[0], r[1]))\n",
        "            else:\n",
        "                print('what')\n",
        "        hpt_maps.append((np.mean(hpt_average_precisions), (hidden_channels, batch_size, lr)))\n",
        "        return np.mean(hpt_average_precisions)\n",
        "    sampler = optuna.samplers.RandomSampler(seed=111)\n",
        "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
        "    study.optimize(objective, n_trials=40)\n",
        "    hpt_results.append(hpt_maps)\n",
        "    _, params = sorted(hpt_maps)[-1]\n",
        "    model = PeptideGNN(params[0], input_channels=128).cuda()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=params[2])\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.5, reduction='none')\n",
        "    print(validation_peptide)\n",
        "    for epoch in range(1, 30):\n",
        "        training_graphs = random.Random(epoch+111).choice(trainings)\n",
        "        train_loader = DataLoader(training_graphs, batch_size=params[1], shuffle=True)\n",
        "        loss = train_weighted(model, criterion, optimizer, train_loader)\n",
        "        if epoch % 14 == 0:\n",
        "            test_acc = test_without_crash(model, criterion, test_loader)\n",
        "            print(f'Epoch: {epoch:02d}, Train Acc: {test_without_crash(model, criterion, train_loader):.4f}, Test AUC: {test_acc:.4f}')\n",
        "\n",
        "    torch.save(model.state_dict(), f'{save_to}pretrained_{validation_peptide[0]}.pth')\n",
        "\n",
        "    model.eval()\n",
        "    all_logits = []\n",
        "    atrues = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            logits = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            all_logits.append(logits.cpu().detach()[:,1])\n",
        "            atrues.append(data.y.cpu())\n",
        "\n",
        "    test_logits.append(np.concatenate(all_logits))\n",
        "    test_labels.append(np.concatenate(atrues))\n",
        "\n",
        "    val_gpcr = [g.gpcr for g in validation_graphs]\n",
        "    val_peptide = [g.peptide for g in validation_graphs]\n",
        "    result = pd.DataFrame(zip(test_labels[-1], test_logits[-1], val_gpcr, val_peptide))\n",
        "    for gpcr, r in result.groupby(2):\n",
        "        hitmapgnn.append((gpcr, r.sort_values(by=1).iloc[-17:][0].sum()))\n",
        "        candidatesgnn.append((gpcr,r.sort_values(by=1)))\n",
        "        if r[0].sum() > 0:\n",
        "            average_precisions[gpcr] = sklearn.metrics.average_precision_score(r[0], r[1])\n",
        "            print(gpcr, average_precisions[gpcr])\n",
        "\n",
        "print(roc_auc_score(np.concatenate(test_labels), np.concatenate(test_logits)))"
      ],
      "metadata": {
        "id": "d3Ti8FEVaBrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(list(average_precisions.values()))"
      ],
      "metadata": {
        "id": "NZzHAo4eOUPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_value = int(round(np.mean(list(average_precisions.values())),3)*1000)\n",
        "st = pd.concat([st for g,st in candidatesgnn])\n",
        "st.to_csv(f'/content/average_precision_values.csv', index=False)"
      ],
      "metadata": {
        "id": "og603gThO1Vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}